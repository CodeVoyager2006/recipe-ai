{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjrFUUpWo8z9tcoFBZlWWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CodeVoyager2006/recipe-ai/blob/main/GenAI_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gen A.I Assignment\n",
        "By: Minh Son Truong/shen.truong2017@gmail.com\n",
        "##Setup Phase\n",
        "import the important libraries from Huggingface"
      ],
      "metadata": {
        "id": "B0xKI0MaC9fZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hulrUnVuC8OS"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the method and test the model"
      ],
      "metadata": {
        "id": "dwx1w8jgDDOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recipe(text):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "  recipe_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "  return tokenizer.decode(recipe_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "L_bf9P1lDJBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a sample text for summarization\n",
        "sample_text =     \"\"\"\n",
        "chicken, pasta, tomato sauce\n",
        "\"\"\"\n",
        "\n",
        "# Summarize the sample text using the pre-trained model (without fine-tuning)\n",
        "pre_finetuned_recipe_generation = recipe(sample_text)\n",
        "print(\"recipe generation before fine-tuning:\", pre_finetuned_recipe_generation)"
      ],
      "metadata": {
        "id": "-g_SNeAbDL7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load dataset\n",
        "- First import the library needed to load the dataset from hugging face\n",
        "\n",
        "\n",
        "- Then, load the database that will be used"
      ],
      "metadata": {
        "id": "CaiFr3rKDXsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the CNN/DailyMail dataset, which contains articles and summaries\n",
        "dataset = load_dataset(\"Shengtao/recipe\", split=\"train\")"
      ],
      "metadata": {
        "id": "zWLtMJ3mDVif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize\n",
        "tokenize the necessary data from the database"
      ],
      "metadata": {
        "id": "xSwJ3lthDnyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "  # Extract the articles from the dataset\n",
        "  inputs = [ingredient for ingredient in examples['ingredients']]\n",
        "\n",
        "  # Tokenize the articles (inputs) with padding and truncation to a max length of 512\n",
        "  model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Tokenize the summaries (labels) using the target tokenizer context\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(examples['title'], max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  # Attach the tokenized summaries as labels to the model inputs\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  # Move the tokenized inputs and labels to the appropriate device (GPU/CPU)\n",
        "  model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "  return model_inputs"
      ],
      "metadata": {
        "id": "6WsFECUlDsWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the small training dataset\n",
        "tokenized_train_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Iw7Ib4NMDtMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up training arguments\n",
        "Give the training model specific configurations as I like"
      ],
      "metadata": {
        "id": "IVdrMM1cDu3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',              # Directory to save the model checkpoints\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate the model at the end of every epoch\n",
        "    learning_rate=2e-5,                  # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=8,       # Batch size for training\n",
        "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
        "    weight_decay=0.01,                   # Regularization to prevent overfitting\n",
        "    save_total_limit=3,                  # Only keep the last 3 checkpoints\n",
        "    num_train_epochs=3,                  # Number of training epochs\n",
        "    predict_with_generate=True,          # Enable text generation during evaluation\n",
        "    logging_dir=\"./logs\"                 # Directory for storing training logs\n",
        ")"
      ],
      "metadata": {
        "id": "O6zq7JEOD05j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the trainer object\n",
        "Using the Seq2SeqTrainer, we can train our model"
      ],
      "metadata": {
        "id": "V11AZaqND8eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# Create the trainer object\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                         # The model to be trained\n",
        "    args=training_args,                  # The training arguments defined earlier\n",
        "    train_dataset=tokenized_train_dataset,  # The tokenized training dataset\n",
        "    tokenizer=tokenizer                  # The tokenizer to handle input and output\n",
        ")"
      ],
      "metadata": {
        "id": "daSd-0x7D3xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's train\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "MoKxaodPD67H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "Evaluate the metrics belong to our model post train"
      ],
      "metadata": {
        "id": "5bcngetHEARa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the evaluation dataset\n",
        "metrics = trainer.evaluate()\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(metrics)"
      ],
      "metadata": {
        "id": "0u9l_PcfEAuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final test\n",
        "- Set up the recipe method again to be called\n",
        "- Run test"
      ],
      "metadata": {
        "id": "1lH4ysGlEC52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recipe(text):\n",
        "  inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "  recipe_ids = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
        "  return tokenizer.decode(recipe_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(recipe(\"\"\"\n",
        "1 (8 ounce) box elbow macaroni ; ¼ cup butter ; ¼ cup all-purpose flour ; ½ teaspoon salt ; ground black pepper to taste ; 2 cups milk ; 2 cups shredded Cheddar cheese\n",
        "\"\"\"))"
      ],
      "metadata": {
        "id": "Uh-SucppEEqG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}